[cite_start]Here is a chapter-wise analysis of predicted questions for your Winter 2025 exam, based on the syllabus [cite: 237, 238] and a frequency analysis of the six provided past papers.

The probability is calculated based on the frequency of the topic appearing in the last six exams (e.g., a topic appearing in all 6 papers has a probability of 1.0). Within each chapter, questions are sorted from most probable to least probable.

---

### ðŸ“Š Predicted Questions by Syllabus Module

[cite_start]**Chapter 1: Basics of Probability, Random Processes and Linear Algebra** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Find the **Eigenvalues and Eigenvectors** for a given 2x2 matrix. [e.g., cite: 15, 123, 153, 166, 196] | 04 | 0.9 |
| Define "Pattern Recognition" and discuss its key steps (e.g., feature extraction) and/or applications. [e.g., cite: 25, 101, 123, 189] | 07 | 0.8 |
| Differentiate between **Autocorrelation and Cross-correlation**. [e.g., cite: 39, 101, 123, 218] | 03 | 0.8 |
| Solve a numerical problem using **Bayes' theorem** (e.g., the defective items/manufacturer problem). [e.g., cite: 20] | 04 | 0.2 |

---

[cite_start]**Chapter 2: Bayes Decision Theory** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Explain **Bayes' Decision Theory** for classification. [e.g., cite: 101, 153, 166, 203] | 07 | 0.6 |
| Explain **Minimum-Error-Rate Classification** (and derive the classifier). [e.g., cite: 101, 123, 166, 206] | 03 or 07 | 0.6 |
| Define and explain: Discriminant functions and Decision surfaces. [e.g., cite: 101, 153] | 04 or 07 | 0.4 |

---

[cite_start]**Chapter 3: Parameter Estimation Methods** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Explain **Maximum Likelihood (ML) Estimation** (often for the Gaussian case). [e.g., cite: 29, 101] | 07 | 0.4 |
| Explain **Bayesian Parameter Estimation** (general cases). [e.g., cite: 33] | 07 | 0.2 |
| Explain **Maximum a Posteriori (MAP)** estimation. [e.g., cite: 123] | 03 | 0.2 |

---

[cite_start]**Chapter 4: Unsupervised learning and clustering** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Explain the **K-Means clustering algorithm** with steps and an example. Discuss its limitations. (May be a numerical problem). [e.g., cite: 53, 101, 123, 153, 166, 199] | 07 | **1.0** |
| Explain the **Expectation-Maximization (EM)** method for parameter estimation (often with GMM). [e.g., cite: 59, 101, 123, 215] | 07 | 0.8 |
| Explain the **K-Nearest Neighbour (KNN)** method. Why is it called a "lazy learner"? [e.g., cite: 101, 124, 153, 166, 225] | 04 or 07 | 0.9 |
| What is the significance of **Hidden Markov Models (HMM)**? / Differentiate between Discrete and Continuous HMMs. [e.g., cite: 76, 84, 101, 127, 153, 166, 222] | 04 | **1.0** |
| Explain **Hierarchical clustering** methods (e.g., agglomerative, divisive). [e.g., cite: 123, 153, 225] | 07 | 0.6 |
| Enlist and explain **criterion functions for clustering**. [e.g., cite: 123, 208] | 04 or 07 | 0.4 |
| Discuss **cluster validation methods**. [e.g., cite: 58] | 04 | 0.2 |

---

[cite_start]**Chapter 5: Dimensionality reduction** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Explain the **Principal Component Analysis (PCA)** method for dimensionality reduction. (May be a numerical problem). [e.g., cite: 72, 87, 101, 123, 153, 166, 211] | 07 | **1.0** |
| Explain **Fisher's Linear Discriminant Analysis (LDA)**. [e.g., cite: 123, 153, 220] | 07 | 0.6 |
| Explain **Dictionary Learning methods** (e.g., Factor Analysis or Non-negative matrix factorization). [e.g., cite: 36, 101, 104, 123, 225] | 07 | 0.6 |
| Find the **Singular Value Decomposition (SVD)** for a given matrix. [e.g., cite: 49] | 03 | 0.2 |

---

[cite_start]**Chapter 6: Linear discriminant functions** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Explain classification using **Support Vector Machines (SVM)**. Define terms like *maximal margin*, *support vectors*, and *kernel functions*. [e.g., cite: 101, 123, 153, 166, 225] | 07 | **1.0** |
| Explain the working of a **Perceptron** (with a diagram). / Differentiate single-layer vs. multi-layer perceptron. [e.g., cite: 57, 123, 166, 192] | 03 or 07 | 0.8 |
| What is the role of **'C' in SVM**? How does it affect the bias/variance trade-off? [e.g., cite: 51] | 04 | 0.2 |
| Explain the principle of **Gradient Descent**. [e.g., cite: 101, 132, 225] | 03 | 0.6 |

---

[cite_start]**Chapter 7: Artificial neural networks** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| With a neat diagram, discuss the topology/architecture of a **Multilayer Feed-Forward Neural Network**. [e.g., cite: 80, 101, 123, 153, 166, 225] | 07 | **1.0** |
| Differentiate between **Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN)**. [e.g., cite: 69, 101, 153, 225] | 04 | 0.8 |
| Describe the architecture of a **CNN** with a diagram, explaining operations like *convolution* and *pooling*. [e.g., cite: 101, 135, 153, 166] | 07 | 0.8 |
| What is a **deep neural network**? Explain its benefits. [e.g., cite: 105, 166, 225] | 04 | 0.6 |

---

[cite_start]**Chapter 8: Non-metric methods for pattern classification** [cite: 238]
| Predicted Question | Typical Marks | Probability |
| :--- | :--- | :--- |
| Explain the **CART (Classification and Regression Trees)** algorithm for decision tree construction. [e.g., cite: 75, 106, 138, 166, 225] | 07 | 0.9 |
| What is **pruning** in decision tree construction? Explain its significance. [e.g., cite: 85, 126, 225] | 04 | 0.6 |
| What is the **Gini index** and what is its role in the CART construction process? [e.g., cite: 153] | 07 | 0.2 |
